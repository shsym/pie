{
  "model_family": "llama-3.1",
  "models": {
    "8B": {
      "name": "Meta-Llama-3.1-8B",
      "parameters": "8B",
      "architecture": {
        "num_layers": 32,
        "hidden_size": 4096,
        "intermediate_size": 6144,
        "num_attention_heads": 32,
        "num_key_value_heads": 8,
        "vocab_size": 128000,
        "max_position_embeddings": 131072,
        "context_length": 128000,
        "head_size": 128,
        "rope_theta": 500000.0,
        "activation": "silu",
        "normalization": "rms_norm",
        "eps": 1e-5
      },
      "test_configs": {
        "batch_prefill_attention": {
          "num_tokens": [128, 512, 2048],
          "num_query_heads": 32,
          "num_kv_heads": 8,
          "head_size": 128,
          "kv_len": [128, 512, 2048],
          "page_size": 16
        },
        "rms_norm": {
          "num_tokens": [128, 512, 2048],
          "hidden_size": 4096,
          "eps": 1e-5
        },
        "silu_and_mul": {
          "num_tokens": [128, 512, 2048],
          "intermediate_size": 6144
        },
        "rope": {
          "num_tokens": [128, 512, 2048],
          "num_query_heads": 32,
          "num_kv_heads": 8,
          "head_size": 128,
          "rope_theta": 500000.0,
          "rope_factor": 1.0,
          "rope_low_frequency_factor": 1.0,
          "rope_high_frequency_factor": 4.0,
          "max_position_embeddings": 131072
        },
        "softmax": {
          "batch_size": [1, 4, 8],
          "vocab_size": 128000,
          "temperature": 1.0
        },
        "topk_mask_logits": {
          "num_tokens": [1, 4, 8],
          "vocab_size": 128000,
          "k": [1, 5, 10, 50]
        },
        "gemm": {
          "cases": [
            {"name": "qkv_proj", "m": [128, 512, 2048], "n": 4096, "k": 4096, "transa": false, "transb": false, "use_bias": false},
            {"name": "o_proj", "m": [128, 512, 2048], "n": 4096, "k": 4096, "transa": false, "transb": false, "use_bias": false},
            {"name": "gate_up_proj", "m": [128, 512, 2048], "n": 6144, "k": 4096, "transa": false, "transb": false, "use_bias": false},
            {"name": "down_proj", "m": [128, 512, 2048], "n": 4096, "k": 6144, "transa": false, "transb": false, "use_bias": false}
          ]
        },
        "grouped_gemm": {
          "num_groups": [1, 4, 8],
          "m": [128, 512],
          "n": 4096,
          "k": 4096,
          "transa": false,
          "transb": false,
          "use_bias": false
        },
        "embedding_lookup": {
          "num_tokens": [128, 512, 2048],
          "hidden_size": 4096,
          "vocab_size": 128000
        },
        "extract_k_values": {
          "M": [1, 4, 8],
          "N": 128000,
          "k": [1, 5, 10, 50]
        },
        "add_residual": {
          "num_tokens": [128, 512, 2048],
          "hidden_size": 4096
        },
        "append_paged_kv_cache": {
          "num_tokens": [128, 512],
          "num_kv_heads": 8,
          "head_size": 128,
          "page_size": 16,
          "max_num_pages": 1024,
          "batch_size": [1, 4, 8]
        }
      }
    },
    "70B": {
      "name": "Meta-Llama-3.1-70B",
      "parameters": "70B",
      "architecture": {
        "num_layers": 80,
        "hidden_size": 8192,
        "intermediate_size": 12288,
        "num_attention_heads": 64,
        "num_key_value_heads": 8,
        "vocab_size": 128000,
        "max_position_embeddings": 131072,
        "context_length": 128000,
        "head_size": 128,
        "rope_theta": 500000.0,
        "activation": "silu",
        "normalization": "rms_norm",
        "eps": 1e-5
      }
    },
    "405B": {
      "name": "Meta-Llama-3.1-405B",
      "parameters": "405B",
      "architecture": {
        "num_layers": 126,
        "hidden_size": 16384,
        "intermediate_size": 20480,
        "num_attention_heads": 128,
        "num_key_value_heads": 8,
        "vocab_size": 128000,
        "max_position_embeddings": 131072,
        "context_length": 128000,
        "head_size": 128,
        "rope_theta": 500000.0,
        "activation": "silu",
        "normalization": "rms_norm",
        "eps": 1e-5
      }
    }
  }
}